{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The model has not learned the meaning of words, but consider:\n",
        "\n",
        "The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.\n",
        "\n",
        "The structure of the output resembles a playâ€”blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
        "\n",
        "As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2x9TLFd1n2iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview\n",
        "This Jupyter Notebook demonstrates how to generate text using a character-based Recurrent Neural Network (RNN). It uses a dataset of Shakespeare's writings and aims to predict the next character in a given sequence.\n",
        "\n",
        "Learning Objectives\n",
        "Understand the basics of text generation with RNNs.\n",
        "Create training examples and targets for text generation.\n",
        "Build an RNN model for sequence generation using Keras Subclassing.\n",
        "Evaluate the text generated by the model.\n",
        "Prerequisites\n",
        "Python 3.x\n",
        "TensorFlow\n",
        "NumPy\n",
        "How to Use\n",
        "Download Dataset: The notebook will automatically download Shakespeare's writings from a predefined URL.\n",
        "\n",
        "Preprocess Data: Tokenize the text data and prepare it for training.\n",
        "\n",
        "Build the Model: Code for constructing the RNN using Keras is provided.\n",
        "\n",
        "Train the Model: Execute the training loop.\n",
        "\n",
        "Generate Text: Use the trained model to generate text."
      ],
      "metadata": {
        "id": "1IfBdSvXoGWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qve1-wT2oIIo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYBzUBCc7ADL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYNE1CZJ9Nxn",
        "outputId": "ad2e9166-03fa-4c04-b692-7543927838cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObRGlZs79Pi6",
        "outputId": "925707e5-2d8c-4086-90e6-af7208e10f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV_q2_5V9VC-",
        "outputId": "ec675e02-3067-4f00-fa27-9bbf411a0f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhjcOYzY9cLM",
        "outputId": "d5ecc32a-f19d-4807-c058-ebd3322f55b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9aMJd2K9d9u",
        "outputId": "944a11f1-ad92-4cdd-fda5-9891c24eb31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "vrRzeM3V9gWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "097-g4uv9_T4",
        "outputId": "a2c060b8-474a-4e8b-a5ec-3c5541b26c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "K7cYMSrD-D-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhziTBTu-ONv",
        "outputId": "ba09ffbb-c763-4b10-de28-68ef6a67266a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CfhtB3i-Sb1",
        "outputId": "a886d1a5-3281-4fbd-fe74-e9780dec7c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "40QWljKn-WIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 2\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, \"UTF-8\"))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgrnqpMg-fES",
        "outputId": "7f6e55c3-5789-4cee-f053-90df57f21ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "xjtEkmR8-h-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgnHEVOA-h69",
        "outputId": "14ca359c-6b81-453d-f26d-0155823644c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)"
      ],
      "metadata": {
        "id": "EhiJEBw4-h5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "    print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTzcek8D-hzh",
        "outputId": "c2a5a008-2289-4172-e7b0-6fea3af1cd06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Onsoy97u-hwo",
        "outputId": "d9b34335-0398-450b-c365-887365413aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "UozYeJvS-htT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlE_0md5-hrV",
        "outputId": "9d9a6699-2d8b-4e65-e878-ffb44b5495cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "-wrepTfO-hgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr31_4Sk-hTM",
        "outputId": "310ced33-804f-408c-ba1b-d234869519af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset.shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YjMxUHl_J-F",
        "outputId": "b57a7592-44df-435e-cdde-17c6fd566174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "81RhexmN_O3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "        # TODO - Create an embedding layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # TODO - Create a GRU layer\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            rnn_units, return_sequences=True, return_state=True\n",
        "        )\n",
        "        # TODO - Finally connect it with a dense layer\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = self.embedding(inputs, training=training)\n",
        "        # since we are training a text generation model,\n",
        "        # we use the previous state, in training. If there is no state,\n",
        "        # then we initialize the state\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x"
      ],
      "metadata": {
        "id": "HKdN0v6MASWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        ")"
      ],
      "metadata": {
        "id": "x43hpTp6AX9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(\n",
        "        example_batch_predictions.shape,\n",
        "        \"# (batch_size, sequence_length, vocab_size)\",\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atadbqHUAb5B",
        "outputId": "f2980ac2-41a0-469c-c52d-7314e651d590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wpKLLDOAha_",
        "outputId": "ba59de62-bef0-4c89-a47a-a10358e7bf24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(\n",
        "    example_batch_predictions[0], num_samples=1\n",
        ")\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "xbVpMwe4AmPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W1xyC1MAqYo",
        "outputId": "b25d6c5e-db68-4cb3-c68c-23286c2a91e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([54, 37, 57, 12, 16, 24, 12, 16, 58,  2, 54, 20, 15, 25, 16, 36, 52,\n",
              "       16,  5, 62, 33, 60, 57, 47, 46, 30,  1, 18, 23, 43, 46, 31, 39, 26,\n",
              "       33, 33, 65, 57,  6, 37, 11, 28,  7, 10, 51, 42, 19, 21, 34, 20, 61,\n",
              "       56, 47,  6,  2, 23, 25, 14, 48, 47, 29, 34, 59, 54, 17, 15, 15, 60,\n",
              "       36, 53, 28, 61, 22, 18, 57, 45, 53, 31, 56, 12, 60, 37,  0, 28,  9,\n",
              "       40, 58, 59,  1, 63, 12, 61, 57, 47, 32, 13, 37, 32, 65, 59])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnxBRpX0Auo7",
        "outputId": "29422ccc-3a2b-441e-f8cb-d7d4e85f3794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'tleman:\\nAy, and more.\\n\\nLUCIO:\\nA French crown more.\\n\\nFirst Gentleman:\\nThou art always figuring diseas'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"oXr;CK;Cs oGBLCWmC&wTurhgQ\\nEJdgRZMTTzr'X:O,3lcFHUGvqh' JLAihPUtoDBBuWnOvIErfnRq;uX[UNK]O.ast\\nx;vrhS?XSzt\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - add a loss function here\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "R7Il_9nwA0Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\n",
        "    \"Prediction shape: \",\n",
        "    example_batch_predictions.shape,\n",
        "    \" # (batch_size, sequence_length, vocab_size)\",\n",
        ")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8mXaBgfA4JT",
        "outputId": "87e3e02d-3681-49f0-9a99-fe6371d098e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1878023, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7oF5744A8_K",
        "outputId": "ee2eac49-575e-4bae-a838-b9a872e130c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.87785"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=loss)"
      ],
      "metadata": {
        "id": "sueVhK6EBEjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = \"./training_checkpoints\"\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix, save_weights_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "bJgTNl00BGFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "6A_FenscBKvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKuDydY7BReE",
        "outputId": "a9a5c500-6fee-4785-a7f4-a198b97dae36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 979s 6s/step - loss: 2.7029\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 949s 6s/step - loss: 1.9756\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 942s 5s/step - loss: 1.7002\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 940s 5s/step - loss: 1.5420\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 943s 5s/step - loss: 1.4458\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 939s 5s/step - loss: 1.3780\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 971s 6s/step - loss: 1.3266\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 950s 6s/step - loss: 1.2815\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 940s 5s/step - loss: 1.2411\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 935s 5s/step - loss: 1.2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "\n",
        "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "        skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            # Put a -inf at each bad index.\n",
        "            values=[-float(\"inf\")] * len(skip_ids),\n",
        "            indices=skip_ids,\n",
        "            # Match the shape to the vocabulary\n",
        "            dense_shape=[len(ids_from_chars.get_vocabulary())],\n",
        "        )\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        # Convert strings to token IDs.\n",
        "        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "        # Run the model.\n",
        "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "        predicted_logits, states = self.model(\n",
        "            inputs=input_ids, states=states, return_state=True\n",
        "        )\n",
        "        # Only use the last prediction.\n",
        "        predicted_logits = predicted_logits[:, -1, :]\n",
        "        predicted_logits = predicted_logits / self.temperature\n",
        "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "        predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "        # Sample the output logits to generate token IDs.\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "        # Convert from token ids to characters\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "        # Return the characters and model state.\n",
        "        return predicted_chars, states"
      ],
      "metadata": {
        "id": "MziP4I8-BRax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "Fshx_QqdBRY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4aHS7zABRV1",
        "outputId": "c3128120-137c-4dfd-9044-066916434feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The reoses, please him-shart, your offorning ill refend me speak\n",
            "At any ofty borne up, looks you.\n",
            "\n",
            "HORTENSIO:\n",
            "I think the woe\n",
            "that was your friend to the glove, within,\n",
            "With his enemy, many friends?\n",
            "\n",
            "ANTONIO:\n",
            "And gentle question is much added\n",
            "The flainth on my hoars to fly.\n",
            "Come, smakes, unfeigntly forbids, not disciplined\n",
            "Unconsinged tood; and so your hon.\n",
            "\n",
            "BENVOLIO:\n",
            "Lendon me to my garder, and I fled home,\n",
            "That Pluased with like a little like an answer\n",
            "By us, the hatable hither braves in\n",
            "PRought are never earth'd in BArJMNTHAM:\n",
            "Renown me, friend, thou with-bold fair there,\n",
            "Seeking the fetched of the tumple your lordship bigle\n",
            "To speak no more behalf his sake, and not too lay tears with strength to measure\n",
            "With our accusation to the queen's\n",
            "fashice, for a man could bear for me.\n",
            "\n",
            "KING HENRY VI:\n",
            "Ay, brave nothing, bods! I will unto your honour\n",
            "Than you ron, will our need with haste.\n",
            "\n",
            "VOLUMNIA:\n",
            "That's he,--\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "God sir, good!\n",
            "\n",
            "GLOUCESTER:\n",
            "Alse me! ancient hambly, old;\n",
            "And t \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 10.733636379241943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8GjRiYpBRUC",
        "outputId": "29c2e929-4a75-4f7c-f2b0-4c763e5d6dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nI too much lur chaste Edward's bower.\\n\\nCLARENCE:\\nIn my reasols after the sun be revenged\\nWithall have fought you to a wine were thousand done:\\nAnd,-for conservant and years out and write and good friend.\\nHow up the untertake the belly's age,\\nWe returous in her colour'd head and flee\\nDraws now to the crave belchmen, conserzard\\nCould not have less me in thy sword; whose field\\nThe batts and vas the earthing and will to you;\\nOr where is a great master's topann, I knew dewise him.\\nSpeak not, I say, our parter sland; first Claudio.\\n\\nMessenger:\\nSo many your queen, take up this land, the like softicer\\nAnd blood in a plague o' the onation\\nmore corpey'd in all accomplish'd yield.\\n\\nLUCIO:\\nAy well for beaten, his awfrive Sinn'd bed!\\n\\nGLOUCESTER:\\nWhen gail like caren, ridy?\\nThe nurse didst be my honour of thine eye,\\nDescrive and store their will, and not to know sucitching,\\nMistress'd in that laugh'd artitle,\\nAnd, lewd and greated field\\nMy flight of your done, and earth forfile,\\nBesude'd the flowe\"\n",
            " b\"ROMEO:\\nPray, below him a two; and I repear-revery way\\nthe nurse you in this fight breathe our strewest,\\nAnd princes assist our malicions seys of Engrood:\\nFor even he not in streeg more word.\\n\\nVARGILIA:\\nNeither bid thee from his rating, forswear\\nAs free ass his well: he shall be his passes to her.\\n\\nPOMPEY:\\nTake hell.\\n\\nLADY GREY:\\nNo? now atieut?\\n\\nBISHOP OF ERCO:\\nWhy, that's father, madam.\\n\\nGREMIO:\\nI am help of York; and with my exiles I pluck thee\\ngo thy hand and far in'to lock his sleevest?\\n\\nFLORIZER:\\nLond maye him seen\\nthine eeferenc.\\n\\nKATH:\\nWho knows now, no; hook send for his life, I'll 't:\\n'Ty's brother gafe, the crawk of care Ifate?\\nIs use your suns; and I an eye declines,\\nAnd the land was out-bride is proportion't, most death,\\nWhich your visit two, than 'tis:\\nWith thou are the truth,\\nIn by the earth, if wild thou hast seen, my knees affecting.\\n\\nCAPULET:\\nService our fairing satrencess!\\nBut since I capt me nothing,\\nThat which I seem to your chowner strong.\\nAnd come your soul is come nor w\"\n",
            " b\"ROMEO:\\nWhy, Welcome you, my lord!\\n\\nGLOUCESTER:\\nYord wouldst thou ne'er will acturalte?\\nCame fortune famors, love against him;\\nAnd sit as well garland's death,\\nThat virture-blood whence he offenders in his\\nfriend you to a name of kings by Calisht.\\nShames the lords of Ludot can spoke\\nA furry and forward friar, and fareden?\\nUncerrity riches, Immostraves me not?\\nMen Move stand in doth fill the happying\\nstrength: then, 'tis would born to me.\\nA scropp-int now in this bride in my too hither,\\nWho madam keeps there and fountage of spring,\\nSpit doth help I wap af Lancaster.\\nThis ofterning, that fills were a botten\\npurpose.\\n\\nPOMPEY:\\nOnce thou thieves the same quick, whison how to wize\\nThe hurt and flower of demosition.\\n\\nFurstic:\\nHere's the nature of my heart prop'd,\\nConfinis' shortly, the hedge of soul of Nupol,\\nAnd minutes lance with consul, condemn'd back for the tame,\\nHenceforth his sake to the prince my heart, the cunnuin\\nTo the other hours our subjects, and will hither,\\nAnd with Gues have left fro\"\n",
            " b\"ROMEO:\\nI no bost here?\\n\\nLieins:\\nBy Jove, it ofitation see him hence,\\nAnd you are forbiding to your wretched hand;\\nAnd sit against selfsage here at all her sight,\\nSo many years to furnish'd upon the war\\nBetwerning, as well to-day; green Dark against\\nMakes them angry beauth will do not, like a plantary.\\n\\nConos:\\nLords, belpet with tears. And take up men's fast and like\\nto lame it from Protecour.\\n\\nSecond Citizen:\\nNow thou woman's fortune,\\nTwice being devives the best field, the senaten brought behaved,\\nBeinu in our parting\\nProve my monay sun: but if you joy\\nShall have her hands no velgar-pleasure.\\n\\nHENRY BOLINGBROKE:\\nCall him did Retent and could I slow to wranch.\\nWhat said the whole way: I deed into resolve;\\nI'll be put mercy. Here he hath proud appointed\\nWith Claudio's prince; will you go a'such for thy day:\\nHealty mind, do not belike,\\nAnd therefore any slaughter too;\\nHe-child i' the end, these glory rade\\nThe very month had quible thee time of\\n\\nVARCIUS:\\n'Tis catered, my dearest friend,\\nAnd tha\"\n",
            " b\"ROMEO:\\nThe nord more govern'd wife his blest here\\nIs not her conjuctions\\nThat in your pishow sad I see cheer this,\\nWhich in thy oft griefs, but they may strike,\\nTo prevent this world and our.\\nGo I, the laugh else ere I seek from him\\nFrom All I to fiend so long lost.\\nAnd here revengeful house apperial him,\\nNor ceased as his behold.\\n\\nANTONIO:\\nLeave be at too half of your great dagger.\\n\\nOXFORD:\\nWhy will let him an easy service to eye,\\nCoriolanus pilecance 'MIRGIN EDIZAND:\\nFare, I'll tell; what say myself to them?\\n\\nBENVOLIO:\\nIs he made bid your pitch toe.\\nWhat servish your will, my lord, he is a figght\\nsubscable and follow'd: and truly\\nHe'll partuade them joyful,\\nFor make haste meet ye note: we were Henry's death,\\nsweet Somerset, on 'Am, how long,\\nGold keep next: look upon her.\\n\\nNORTHUMBERLAND:\\nThe Senate, as mend, that he should be.\\n\\nBENVOLIO:\\nIf he durdend knows the horsements of such death-way.\\n\\nMAMILLIUS:\\nNot believe in arusonice!\\n'But I but for my older war I will\\nFollow us follion for my f\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 6.4058380126953125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, \"one_step\")\n",
        "one_step_reloaded = tf.saved_model.load(\"one_step\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkJN4Uq1BRRQ",
        "outputId": "84d062d0-b2e9-489d-ef60-8ac64ae1a627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x79bed85d7be0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "    next_char, states = one_step_reloaded.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBD1nhw_BRPV",
        "outputId": "6ae2f915-c30b-498d-f54d-85ec4bc733a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Help to her friend Drunker Without mend.\n",
            "To the account wrench fill, allow, when bloods,\n",
            "Brother in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "    @tf.function\n",
        "    def train_step(self, inputs):\n",
        "        inputs, labels = inputs\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True)\n",
        "            loss = self.loss(labels, predictions)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        return {\"loss\": loss}"
      ],
      "metadata": {
        "id": "JXhtS5egBRMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        ")"
      ],
      "metadata": {
        "id": "M4OUHP8fBRJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        ")"
      ],
      "metadata": {
        "id": "rhsN2xeEBRGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP94Sn74BREm",
        "outputId": "77024ec2-73f0-4a8c-c9b8-99d03bf19cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 1001s 6s/step - loss: 2.7395\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79beca2ea8f0>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for batch_n, (inp, target) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs[\"loss\"])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = (\n",
        "                f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            )\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f\"Epoch {epoch+1} Loss: {mean.result().numpy():.4f}\")\n",
        "    print(f\"Time taken for 1 epoch {time.time() - start:.2f} sec\")\n",
        "    print(\"_\" * 80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Dtm-twICBO0",
        "outputId": "b727b195-4d16-4312-85a7-e839cdb733de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1831\n",
            "Epoch 1 Batch 50 Loss 2.0393\n",
            "Epoch 1 Batch 100 Loss 1.9791\n",
            "Epoch 1 Batch 150 Loss 1.8869\n",
            "\n",
            "Epoch 1 Loss: 2.0010\n",
            "Time taken for 1 epoch 981.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8326\n",
            "Epoch 2 Batch 50 Loss 1.7218\n",
            "Epoch 2 Batch 100 Loss 1.6687\n",
            "Epoch 2 Batch 150 Loss 1.6378\n",
            "\n",
            "Epoch 2 Loss: 1.7222\n",
            "Time taken for 1 epoch 981.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6366\n",
            "Epoch 3 Batch 50 Loss 1.5801\n",
            "Epoch 3 Batch 100 Loss 1.5679\n",
            "Epoch 3 Batch 150 Loss 1.5132\n",
            "\n",
            "Epoch 3 Loss: 1.5595\n",
            "Time taken for 1 epoch 981.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4991\n",
            "Epoch 4 Batch 50 Loss 1.4769\n",
            "Epoch 4 Batch 100 Loss 1.4618\n",
            "Epoch 4 Batch 150 Loss 1.4545\n",
            "\n",
            "Epoch 4 Loss: 1.4585\n",
            "Time taken for 1 epoch 981.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3809\n",
            "Epoch 5 Batch 50 Loss 1.3816\n",
            "Epoch 5 Batch 100 Loss 1.3833\n",
            "Epoch 5 Batch 150 Loss 1.4025\n",
            "\n",
            "Epoch 5 Loss: 1.3899\n",
            "Time taken for 1 epoch 982.10 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3253\n",
            "Epoch 6 Batch 50 Loss 1.3776\n",
            "Epoch 6 Batch 100 Loss 1.3064\n",
            "Epoch 6 Batch 150 Loss 1.2921\n",
            "\n",
            "Epoch 6 Loss: 1.3360\n",
            "Time taken for 1 epoch 981.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2770\n",
            "Epoch 7 Batch 50 Loss 1.2971\n",
            "Epoch 7 Batch 100 Loss 1.3107\n",
            "Epoch 7 Batch 150 Loss 1.3182\n",
            "\n",
            "Epoch 7 Loss: 1.2922\n",
            "Time taken for 1 epoch 961.29 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2456\n",
            "Epoch 8 Batch 50 Loss 1.2578\n",
            "Epoch 8 Batch 100 Loss 1.2160\n",
            "Epoch 8 Batch 150 Loss 1.2652\n",
            "\n",
            "Epoch 8 Loss: 1.2502\n",
            "Time taken for 1 epoch 981.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.2168\n",
            "Epoch 9 Batch 50 Loss 1.2210\n",
            "Epoch 9 Batch 100 Loss 1.2089\n",
            "Epoch 9 Batch 150 Loss 1.2205\n",
            "\n",
            "Epoch 9 Loss: 1.2115\n",
            "Time taken for 1 epoch 981.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1483\n",
            "Epoch 10 Batch 50 Loss 1.1534\n",
            "Epoch 10 Batch 100 Loss 1.1829\n",
            "Epoch 10 Batch 150 Loss 1.2008\n",
            "\n",
            "Epoch 10 Loss: 1.1727\n",
            "Time taken for 1 epoch 982.06 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}